{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUSTOMER SATISFACTION:  \n",
    "## SANDANDER BANKING DATASET\n",
    "## By RafaBaca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL PRESENTATION: \n",
    "\n",
    "## MULTILAYER PERCEPTRON (MLP) vs XG Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comparative analysis between MLP Neural Net and XG Boost machine learning models.  \n",
    "An exploration of MLP and XG Boost model each applied to a SKLearn environment and Tensorflow environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset -fs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](NYSEJune12017.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Customer satisfaction is a key measure of success. Unhappy customers don't stick around. What's more, unhappy customers rarely voice their dissatisfaction before leaving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer satisfaction is a key measure of success. Unhappy customers don't stick around. What's more, unhappy customers rarely voice their dissatisfaction before leaving.  A global, bank SANTANDER BANK has supplied a dataset help them identify dissatisfied customers early in their relationship. Doing so would allow Santander to take proactive steps to improve a customer's happiness before it's too late.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Santander. is Spanish bank headquartered in Santander, Spain with €1.34 trillion in assets, that includes its Boston-based U.S. subsidiary.  At its 650 branches, Santander serves 100 million customers in Europe, Latin America, and the United States. The bank was first formed in the 1850s in the beach resort town of Santander. The town is also famed for its Paleolithic caveman paintings of European bison nearby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](servicemap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats # may or may not keep\n",
    "import statsmodels # may or may not keep\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from itertools import combinations\n",
    "from numpy import array,array_equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "#make run in ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/x/Desktop/Rafa_NN Proj_2ML '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#directory listing for Active Project notebooks\n",
    "#ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Especially for training set which some online versions are corrupt\n",
    "# Please refer to this git for working versions of the datasets:\n",
    "# https://github.com/caiomsouza/kaggle-competitions/tree/master/santander-customer-satisfaction/dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 371)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAINING DATASET - FEATURE SELECTION #\n",
    "train=pd.read_csv('train.csv')\n",
    "#listings.describe\n",
    "train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count of Rows w/ Missing Values: \n",
    "np.count_nonzero(train.isnull().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39205.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49278.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  var3  var15  imp_ent_var16_ult1  imp_op_var39_comer_ult1  \\\n",
       "0   1     2     23                 0.0                      0.0   \n",
       "1   3     2     34                 0.0                      0.0   \n",
       "\n",
       "   imp_op_var39_comer_ult3  imp_op_var40_comer_ult1  imp_op_var40_comer_ult3  \\\n",
       "0                      0.0                      0.0                      0.0   \n",
       "1                      0.0                      0.0                      0.0   \n",
       "\n",
       "   imp_op_var40_efect_ult1  imp_op_var40_efect_ult3   ...    \\\n",
       "0                      0.0                      0.0   ...     \n",
       "1                      0.0                      0.0   ...     \n",
       "\n",
       "   saldo_medio_var33_hace2  saldo_medio_var33_hace3  saldo_medio_var33_ult1  \\\n",
       "0                      0.0                      0.0                     0.0   \n",
       "1                      0.0                      0.0                     0.0   \n",
       "\n",
       "   saldo_medio_var33_ult3  saldo_medio_var44_hace2  saldo_medio_var44_hace3  \\\n",
       "0                     0.0                      0.0                      0.0   \n",
       "1                     0.0                      0.0                      0.0   \n",
       "\n",
       "   saldo_medio_var44_ult1  saldo_medio_var44_ult3     var38  TARGET  \n",
       "0                     0.0                     0.0  39205.17       0  \n",
       "1                     0.0                     0.0  49278.03       0  \n",
       "\n",
       "[2 rows x 371 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 76020 entries, 0 to 76019\n",
      "Columns: 371 entries, ID to TARGET\n",
      "dtypes: float64(111), int64(260)\n",
      "memory usage: 215.2 MB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](CustSatML.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "#reminder - locally trigger from terminal:  source activate dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation as cv\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model \n",
    "from sklearn import naive_bayes \n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]\n",
    "#).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Any results you write to the current directory are saved as output.\n",
    "def print_shapes():\n",
    "    print('Train: {}\\nTest: {}'.format(train_dataset.shape, test_dataset.shape))\n",
    "train_dataset = pd.read_csv('train.csv', index_col='ID')\n",
    "test_dataset = pd.read_csv('test.csv', index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (76020, 370)\n",
      "Test: (75818, 369)\n"
     ]
    }
   ],
   "source": [
    "print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 nulls in TRAIN and 0 nulls in TEST dataset.\n"
     ]
    }
   ],
   "source": [
    "# How many nulls are there in the datasets?\n",
    "nulls_train = (train_dataset.isnull().sum()==1).sum()\n",
    "nulls_test = (test_dataset.isnull().sum()==1).sum()\n",
    "print('There are {} nulls in TRAIN and {} nulls in TEST dataset.'.format(nulls_train, nulls_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove constant features\n",
    "\n",
    "def identify_constant_features(dataframe):\n",
    "    count_uniques = dataframe.apply(lambda x: len(x.unique()))\n",
    "    constants = count_uniques[count_uniques == 1].index.tolist()\n",
    "    return constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 34 constant features in TRAIN dataset.\n"
     ]
    }
   ],
   "source": [
    "constant_features_train = set(identify_constant_features(train_dataset))\n",
    "\n",
    "print('There were {} constant features in TRAIN dataset.'.format(\n",
    "        len(constant_features_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (76020, 336)\n",
      "Test: (75818, 369)\n"
     ]
    }
   ],
   "source": [
    "#keep nonConstant features but be sure to drop set of constant features\n",
    "train_dataset1=train_dataset\n",
    "\n",
    "# Drop the constant features\n",
    "train_dataset.drop(constant_features_train, inplace=True, axis=1)\n",
    "\n",
    "print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove equal features\n",
    "\n",
    "def identify_equal_features(dataframe):\n",
    "    features_to_compare = list(combinations(dataframe.columns.tolist(),2))\n",
    "    equal_features = []\n",
    "    for compare in features_to_compare:\n",
    "        is_equal = array_equal(dataframe[compare[0]],dataframe[compare[1]])\n",
    "        if is_equal:\n",
    "            equal_features.append(list(compare))\n",
    "    return equal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 29 pairs of equal features in TRAIN dataset.\n"
     ]
    }
   ],
   "source": [
    "equal_features_train = identify_equal_features(train_dataset)\n",
    "\n",
    "print('There were {} pairs of equal features in TRAIN dataset.'.format(len(equal_features_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (76020, 307)\n",
      "Test: (75818, 369)\n"
     ]
    }
   ],
   "source": [
    "# Remove the second feature of each pair.\n",
    "\n",
    "features_to_drop = array(equal_features_train)[:,1] \n",
    "train_dataset.drop(features_to_drop, axis=1, inplace=True)\n",
    "\n",
    "print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the variables for the model.\n",
    "\n",
    "y_name = 'TARGET'\n",
    "feature_names = train_dataset.columns.tolist()\n",
    "feature_names.remove(y_name)\n",
    "\n",
    "X = train_dataset[feature_names]\n",
    "y = train_dataset[y_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected\n",
      "['var3', 'var15', 'imp_ent_var16_ult1', 'imp_op_var39_comer_ult1', 'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1', 'imp_op_var40_comer_ult3', 'imp_op_var40_efect_ult1', 'imp_op_var40_efect_ult3', 'imp_op_var40_ult1', 'imp_op_var41_comer_ult1', 'imp_op_var41_comer_ult3', 'imp_op_var41_efect_ult1', 'imp_op_var41_efect_ult3', 'imp_op_var41_ult1', 'imp_op_var39_efect_ult1', 'imp_op_var39_efect_ult3', 'imp_op_var39_ult1', 'imp_sal_var16_ult1', 'ind_var1_0', 'ind_var1', 'ind_var5_0', 'ind_var5', 'ind_var6_0', 'ind_var6', 'ind_var8_0', 'ind_var8', 'ind_var12_0', 'ind_var12', 'ind_var13_0', 'ind_var13_corto_0', 'ind_var13_corto', 'ind_var13_largo_0', 'ind_var13_largo', 'ind_var13_medio_0', 'ind_var13', 'ind_var14_0', 'ind_var14', 'ind_var17_0', 'ind_var17', 'ind_var18_0', 'ind_var19', 'ind_var20_0', 'ind_var20', 'ind_var24_0', 'ind_var24', 'ind_var25_cte', 'ind_var26_0', 'ind_var26_cte', 'ind_var25_0', 'ind_var30_0', 'ind_var30', 'ind_var31_0', 'ind_var31', 'ind_var32_cte', 'ind_var32_0', 'ind_var33_0', 'ind_var33', 'ind_var34_0', 'ind_var37_cte', 'ind_var37_0', 'ind_var39_0', 'ind_var40_0', 'ind_var40', 'ind_var41_0', 'ind_var44_0', 'ind_var44', 'num_var1_0', 'num_var1', 'num_var4', 'num_var5_0', 'num_var5', 'num_var6_0', 'num_var6', 'num_var8_0', 'num_var8', 'num_var12_0', 'num_var12', 'num_var13_0', 'num_var13_corto_0', 'num_var13_corto', 'num_var13_largo_0', 'num_var13_largo', 'num_var13_medio_0', 'num_var13', 'num_var14_0', 'num_var14', 'num_var17_0', 'num_var17', 'num_var18_0', 'num_var20_0', 'num_var20', 'num_var24_0', 'num_var24', 'num_var26_0', 'num_var25_0', 'num_op_var40_hace2', 'num_op_var40_hace3', 'num_op_var40_ult1', 'num_op_var40_ult3', 'num_op_var41_hace2', 'num_op_var41_hace3', 'num_op_var41_ult1', 'num_op_var41_ult3', 'num_op_var39_hace2', 'num_op_var39_hace3', 'num_op_var39_ult1', 'num_op_var39_ult3', 'num_var30_0', 'num_var30', 'num_var31_0', 'num_var31', 'num_var32_0', 'num_var33_0', 'num_var33', 'num_var34_0', 'num_var35', 'num_var37_med_ult2', 'num_var37_0', 'num_var39_0', 'num_var40_0', 'num_var40', 'num_var41_0', 'num_var42_0', 'num_var42', 'num_var44_0', 'num_var44', 'saldo_var1', 'saldo_var5', 'saldo_var6', 'saldo_var8', 'saldo_var12', 'saldo_var13_corto', 'saldo_var13_largo', 'saldo_var13_medio', 'saldo_var13', 'saldo_var14', 'saldo_var17', 'saldo_var18', 'saldo_var20', 'saldo_var24', 'saldo_var26', 'saldo_var25', 'saldo_var30', 'saldo_var31', 'saldo_var32', 'saldo_var33', 'saldo_var34', 'saldo_var37', 'saldo_var40', 'saldo_var42', 'saldo_var44', 'var36', 'delta_imp_amort_var18_1y3', 'delta_imp_amort_var34_1y3', 'delta_imp_aport_var13_1y3', 'delta_imp_aport_var17_1y3', 'delta_imp_aport_var33_1y3', 'delta_imp_compra_var44_1y3', 'delta_imp_reemb_var13_1y3', 'delta_imp_reemb_var17_1y3', 'delta_imp_reemb_var33_1y3', 'delta_imp_trasp_var17_in_1y3', 'delta_imp_trasp_var17_out_1y3', 'delta_imp_trasp_var33_in_1y3', 'delta_imp_trasp_var33_out_1y3', 'delta_imp_venta_var44_1y3', 'delta_num_aport_var13_1y3', 'delta_num_aport_var17_1y3', 'delta_num_aport_var33_1y3', 'delta_num_compra_var44_1y3', 'delta_num_venta_var44_1y3', 'imp_amort_var18_ult1', 'imp_amort_var34_ult1', 'imp_aport_var13_hace3', 'imp_aport_var13_ult1', 'imp_aport_var17_hace3', 'imp_aport_var17_ult1', 'imp_aport_var33_hace3', 'imp_aport_var33_ult1', 'imp_var7_emit_ult1', 'imp_var7_recib_ult1', 'imp_compra_var44_hace3', 'imp_compra_var44_ult1', 'imp_reemb_var13_ult1', 'imp_reemb_var17_hace3', 'imp_reemb_var17_ult1', 'imp_reemb_var33_ult1', 'imp_var43_emit_ult1', 'imp_trans_var37_ult1', 'imp_trasp_var17_in_hace3', 'imp_trasp_var17_in_ult1', 'imp_trasp_var17_out_ult1', 'imp_trasp_var33_in_hace3', 'imp_trasp_var33_in_ult1', 'imp_trasp_var33_out_ult1', 'imp_venta_var44_hace3', 'imp_venta_var44_ult1', 'ind_var7_emit_ult1', 'ind_var7_recib_ult1', 'ind_var10_ult1', 'ind_var10cte_ult1', 'ind_var9_cte_ult1', 'ind_var9_ult1', 'ind_var43_emit_ult1', 'ind_var43_recib_ult1', 'var21', 'num_aport_var13_hace3', 'num_aport_var13_ult1', 'num_aport_var17_hace3', 'num_aport_var17_ult1', 'num_aport_var33_hace3', 'num_aport_var33_ult1', 'num_var7_emit_ult1', 'num_var7_recib_ult1', 'num_compra_var44_hace3', 'num_compra_var44_ult1', 'num_ent_var16_ult1', 'num_var22_hace2', 'num_var22_hace3', 'num_var22_ult1', 'num_var22_ult3', 'num_med_var22_ult3', 'num_med_var45_ult3', 'num_meses_var5_ult3', 'num_meses_var8_ult3', 'num_meses_var12_ult3', 'num_meses_var13_corto_ult3', 'num_meses_var13_largo_ult3', 'num_meses_var13_medio_ult3', 'num_meses_var17_ult3', 'num_meses_var29_ult3', 'num_meses_var33_ult3', 'num_meses_var39_vig_ult3', 'num_meses_var44_ult3', 'num_op_var39_comer_ult1', 'num_op_var39_comer_ult3', 'num_op_var40_comer_ult1', 'num_op_var40_comer_ult3', 'num_op_var40_efect_ult1', 'num_op_var40_efect_ult3', 'num_op_var41_comer_ult1', 'num_op_var41_comer_ult3', 'num_op_var41_efect_ult1', 'num_op_var41_efect_ult3', 'num_op_var39_efect_ult1', 'num_op_var39_efect_ult3', 'num_reemb_var13_ult1', 'num_reemb_var17_hace3', 'num_reemb_var17_ult1', 'num_reemb_var33_ult1', 'num_sal_var16_ult1', 'num_var43_emit_ult1', 'num_var43_recib_ult1', 'num_trasp_var11_ult1', 'num_trasp_var17_in_hace3', 'num_trasp_var17_in_ult1', 'num_trasp_var17_out_ult1', 'num_trasp_var33_in_hace3', 'num_trasp_var33_in_ult1', 'num_trasp_var33_out_ult1', 'num_venta_var44_hace3', 'num_venta_var44_ult1', 'num_var45_hace2', 'num_var45_hace3', 'num_var45_ult1', 'num_var45_ult3', 'saldo_medio_var5_hace2', 'saldo_medio_var5_hace3', 'saldo_medio_var5_ult1', 'saldo_medio_var5_ult3', 'saldo_medio_var8_hace2', 'saldo_medio_var8_hace3', 'saldo_medio_var8_ult1', 'saldo_medio_var8_ult3', 'saldo_medio_var12_hace2', 'saldo_medio_var12_hace3', 'saldo_medio_var12_ult1', 'saldo_medio_var12_ult3', 'saldo_medio_var13_corto_hace2', 'saldo_medio_var13_corto_hace3', 'saldo_medio_var13_corto_ult1', 'saldo_medio_var13_corto_ult3', 'saldo_medio_var13_largo_hace2', 'saldo_medio_var13_largo_hace3', 'saldo_medio_var13_largo_ult1', 'saldo_medio_var13_largo_ult3', 'saldo_medio_var13_medio_hace2', 'saldo_medio_var13_medio_ult3', 'saldo_medio_var17_hace2', 'saldo_medio_var17_hace3', 'saldo_medio_var17_ult1', 'saldo_medio_var17_ult3', 'saldo_medio_var29_hace2', 'saldo_medio_var29_hace3', 'saldo_medio_var29_ult1', 'saldo_medio_var29_ult3', 'saldo_medio_var33_hace2', 'saldo_medio_var33_hace3', 'saldo_medio_var33_ult1', 'saldo_medio_var33_ult3', 'saldo_medio_var44_hace2', 'saldo_medio_var44_hace3', 'saldo_medio_var44_ult1', 'saldo_medio_var44_ult3', 'var38']\n"
     ]
    }
   ],
   "source": [
    "# Save the features / selected for later use.\n",
    "pd.Series(feature_names).to_csv('features_selected_step1_ensemble.csv', index=False)\n",
    "print('Features selected\\n{}'.format(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Proportion of classes\n",
    "y.value_counts()/len(y)\n",
    "\n",
    "skf = cv.StratifiedKFold(y, n_folds=3, shuffle=True)\n",
    "score_metric = 'roc_auc'\n",
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/x/anaconda/envs/dl/lib/python3.6/site-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 2.526483753593025e-19\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/Users/x/anaconda/envs/dl/lib/python3.6/site-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 3.6929805299417377e-19\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n",
      "/Users/x/anaconda/envs/dl/lib/python3.6/site-packages/scipy/linalg/basic.py:223: RuntimeWarning: scipy.linalg.solve\n",
      "Ill-conditioned matrix detected. Result is not guaranteed to be accurate.\n",
      "Reciprocal condition number: 2.289435365944913e-19\n",
      "  ' condition number: {}'.format(rcond), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "def score_model(model):\n",
    "    return cv.cross_val_score(model, X, y, cv=skf, scoring=score_metric)\n",
    "\n",
    "# time: 10s\n",
    "scores['tree'] = score_model(tree.DecisionTreeClassifier()) \n",
    "\n",
    "# time: 9s\n",
    "scores['extra_tree'] = score_model(ensemble.ExtraTreesClassifier())\n",
    "\n",
    "# time: 7s\n",
    "scores['forest'] = score_model(ensemble.RandomForestClassifier())\n",
    "\n",
    "# time: 33s\n",
    "scores['ada_boost'] = score_model(ensemble.AdaBoostClassifier())\n",
    "\n",
    "# time: 1min\n",
    "scores['bagging'] = score_model(ensemble.BaggingClassifier())\n",
    "\n",
    "# time: 2min30s\n",
    "scores['grad_boost'] = score_model(ensemble.GradientBoostingClassifier())\n",
    "\n",
    "# time: 49s\n",
    "scores['ridge'] = score_model(linear_model.RidgeClassifier())\n",
    "\n",
    "# time: 4s\n",
    "scores['passive'] = score_model(linear_model.PassiveAggressiveClassifier())\n",
    "\n",
    "# time: 4s\n",
    "scores['sgd'] = score_model(linear_model.SGDClassifier())\n",
    "\n",
    "# time: 3s\n",
    "scores['gaussian'] = score_model(naive_bayes.GaussianNB())\n",
    "\n",
    "# time: 4min\n",
    "scores['xgboost'] = score_model(xgb.XGBClassifier())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model scores\n",
      "ada_boost     0.826580\n",
      "bagging       0.704208\n",
      "extra_tree    0.638597\n",
      "forest        0.676940\n",
      "gaussian      0.513835\n",
      "grad_boost    0.834599\n",
      "passive       0.579428\n",
      "ridge         0.791390\n",
      "sgd           0.609369\n",
      "tree          0.571150\n",
      "xgboost       0.837041\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print the scores\n",
    "model_scores = pd.DataFrame(scores).mean()\n",
    "model_scores.sort_values(ascending=False)\n",
    "model_scores.to_csv('model_scores.csv', index=False)\n",
    "print('Model scores\\n{}'.format(model_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORATION INSIGHTS:\n",
    "\n",
    "To arrive at a baseline to compare standard machine learning algorthms with a NN, I chose a stratified baseline.  Accordingly, to prepare the traning data, duplicative and equivalent features were removed from the inital 370 features to a cleaned dataset having about 300 features.  The training dataset had about 76,000 observations with no null values.\n",
    "\n",
    "The purpose of a cross validation exercise is to quantatively guage how well a particualar model will fit with the given data. The above shows a classification baseline where XGBOOST did remarkeably well with the given Santander traning dataset - with a cross validation score of 0.837.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](castleSantander.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP NEURAL NETWORK:\n",
    "#           MULTILAYER PERCEPTRON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, deep learning neural network methods typically require a lot more training data than an XGBoost or other traditional model without the tendency to overfit as the traditional models do with too much data. As the Santander dataset is about 100K obervations, the experimental question becomes is a deep neural network or a tradional model the ideal?\n",
    "\n",
    "In practice, it appears that a network with at least one hidden layer is the \"go to\" approximator.   Generally, adding more hidden layers do not necessarily mean that the neural network model will perform better.  Moreover, neural networks with more hidden layers are prone to overfitting noise on the training data.\n",
    "\n",
    "The following approach was to apply a simple one-hidden layer MLP with provided by the SKLearn package and thereafter apply a two-hidden layer MLP with tensor flow and analyze model performance.  As sklearn showed a great XGBOOST fit above, an XGBoost application using Tensorflow was also attempted for the experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MLP MODEL: \n",
    "An MLP is one example of a feedforward (artificial) neural network where data is directed by process in only one direction (i.e. not a looping cycle) from a layer of input nodes, through at least one hidden layer, and to a layer of output nodes.  Accordingly, each neuron within an input, hidden, and output layer is connected in a one directional or “feed-forward” way.  Each neuron applies an activation function (such as a sigmoid, hyperbolic tangent or softmax) to its received inputs so as to generate a corresponding feedforward output.  Similar to either an electronic gate or trigger rate of a biological neuron cell, an activation function (or “transfer function”) defines the output of any given set of inputs.  It should, however, be emphasized that neurons cannot characteristically fire at a faster rate that is inherently characteristic of such a neuron, therefore the resulting natural network must be differentiable so as to require the calculation of a back propagation for the overall network.\n",
    "\n",
    "Backpropagation is the most popular of learning techniques where output values are compared with actual values to compute some pre-defined error function that is, in turn, fed back through the network (hence the term “backpropagation”).  The error function results are used to iteratively adjust the weights (with each successive training cycle) between each connected neuron so that the network ultimately arrives at a “learned” target function having minimal error calculations.\n",
    "\n",
    "Weight adjustment techniques typically use a non-linear optimizer, such as a gradient decent optimization technique.  In particular, the network calculates the derivative of the error function with respect to the weights to thus change the weights to decrease error - as if going downhill toward a global minimum upon the surface of an error function.  Again, for this reason, it is critical that the network apply differentiable functions to ensure the application of backpropagation.\n",
    "\n",
    "Extra caution should be adhered where only very limited numbers of training samples are available to avoid network overfitting.  Capturing the true statistical process of generating the data breaks down in the event of overfitting.  Moreover, either “stopping” or “drop out” techniques are provided to allow the network to generalize examples not in the training set.  Further caution is commonly exercised when back-propagation algorithms are at the speed of convergence leading to the possibility of end up with an unwanted local minimum of the error function, as opposed to the global minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEURAL NETWORK /DEEP LEARNING BASELINE PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](sklearn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLEARN version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tai/anaconda3/envs/dl/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import xgboost as xgb\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PreProcessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove constant columns\n",
    "# Even more common is the presence of predictors that are almost constant \n",
    "#across samples. One quick and dirty solution is to remove all predictors \n",
    "# that satisfy some threshold criterion related to their variance.\n",
    "remove = []\n",
    "for col in df_train.columns:\n",
    "    if df_train[col].std() == 0:\n",
    "        remove.append(col)\n",
    "\n",
    "df_train.drop(remove, axis=1, inplace=True)\n",
    "df_test.drop(remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove duplicated columns\n",
    "remove = []\n",
    "c = df_train.columns\n",
    "\n",
    "for i in range(len(c)-1):\n",
    "    v = df_train[c[i]].values\n",
    "    for j in range(i+1,len(c)):\n",
    "        if np.array_equal(v,df_train[c[j]].values):\n",
    "            remove.append(c[j])\n",
    "\n",
    "df_train.drop(remove, axis=1, inplace=True)\n",
    "df_test.drop(remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = df_train['TARGET'].values\n",
    "X_train = df_train.drop(['ID','TARGET'], axis=1).values\n",
    "\n",
    "id_test = df_test['ID']\n",
    "X_test = df_test.drop(['ID'], axis=1).values\n",
    "\n",
    "len_train = len(X_train)\n",
    "len_test  = len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(#hidden_layer_sizes=(,128,64,32), \n",
    "\t\t\t\t\tactivation='relu', \n",
    "     \t\t\t\tbeta_1=0.6, \n",
    "     \t\t\t\tbeta_2=0.9,\n",
    "                    alpha = 0.001,\n",
    "                    early_stopping = True,\n",
    "                    shuffle = True,\n",
    "                    warm_start = True,\n",
    "                    validation_fraction = 0.3,\n",
    "     \t\t\t\tlearning_rate_init=0.01, \n",
    "     \t\t\t\tmax_iter = 14000, \n",
    "     \t\t\t\trandom_state = 1235, \n",
    "     \t\t\t\tlearning_rate='adaptive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test pred [ 0.08155128  0.08031955  0.03072938 ...,  0.01636599  0.10643202\n",
      "  0.0180438 ]\n",
      "Overall AUC: 0.801036839088\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred= clf.predict_proba(X_test)[:,1]\n",
    "print('Test pred', y_pred)\n",
    "print('Overall AUC:', roc_auc_score(y_train, clf.predict_proba(X_train)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1590view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TENSORFLOW VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tensorflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "start_time = time.time()\n",
    "tag = str(start_time)\n",
    "np.random.seed(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "DEBUG = False\n",
    "GRAPH = False\n",
    "EPSILON = 1e-12\n",
    "NUM_NEG_EXAMPLES = 73012 # number of negative examples to use; must not exceed 73012\n",
    "                        # (the number of positive examples in the training data is 3008)\n",
    "LEARNING_RATE = 5e-4\n",
    "TRAINING_ITERATIONS = 6000\n",
    "    \n",
    "HIDDEN_LAYER_SIZE = [392,169] #applying two hidden layers\n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 75\n",
    "\n",
    "# set to 0 to train on all available data\n",
    "VALIDATION_SIZE = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read training data from CSV file \n",
    "data = pd.read_csv('train.csv')\n",
    "train_ids_and_targets = data[['ID','TARGET']]\n",
    "\n",
    "pos = data.loc[data['TARGET'] == 1] # \"positive\" examples, i.e., those with TARGET=1\n",
    "neg = data.loc[data['TARGET'] == 0] # \"negative\" training examples...\n",
    "if DEBUG:\n",
    "    print('pos:')\n",
    "    print(pos.head())\n",
    "    print('neg:')\n",
    "    print(neg.head())\n",
    "\n",
    "neg_rows = neg.values[:,1:-1] #discard ID and TARGET\n",
    "pos_rows = pos.values[:,1:-1] #discard ID and TARGET\n",
    "neg_labels = neg.values[:,[0,-1]] #ID and TARGET only\n",
    "pos_labels = pos.values[:,[0,-1]] #ID and TARGET only\n",
    "\n",
    "if DEBUG:\n",
    "    print('neg_rows:',neg_rows.shape)\n",
    "    print('pos_rows:',pos_rows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change of Approach - Don't apply to shuffle at this juncture:\n",
    "### Use k-fold instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle the data before cross validation averts highly correlated data when training NNs\n",
    "# hold on shuffle is all about sequential random batches; k-fold is \"sequence agnostic\"\n",
    "# I now believe sequence agnostic is better at this juncture\n",
    "num_examples = neg.shape[0]\n",
    "perm = np.arange(num_examples) \n",
    "np.random.shuffle(perm)         #random permutation applied\n",
    "neg_rows = neg_rows[perm]       #apply the same permutation to the feature rows\n",
    "neg_labels = neg_labels[perm]   #and the TARGET values\n",
    "if DEBUG:\n",
    "    print('after shuffling:')\n",
    "    print('pos:')\n",
    "    print(pos_rows[:5])\n",
    "    print('neg:')\n",
    "    print(neg_rows[:5])\n",
    "    print(pos_rows.shape,pos_labels.shape)\n",
    "    print(neg_rows.shape,neg_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# restrict training data by discarding all but NUM_NEG_EXAMPLES of the negative examples\n",
    "# but keep all 3008 positive examples\n",
    "data = np.concatenate((pos_rows,neg_rows[:NUM_NEG_EXAMPLES,:]),axis=0)\n",
    "labels = np.concatenate((pos_labels,neg_labels[:NUM_NEG_EXAMPLES,:]),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76020, 369), (76020, 2))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  77.,    1.],\n",
       "       [ 159.,    1.],\n",
       "       [ 220.,    1.],\n",
       "       [ 303.,    1.],\n",
       "       [ 306.,    1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the quartiles on the training data for normalization\n",
    "mu = np.percentile(data,50.0,axis=0)\n",
    "s2 = np.percentile(data,75.0,axis=0)\n",
    "s1 = np.percentile(data,25.0,axis=0)\n",
    "\n",
    "if DEBUG:\n",
    "    print('data, labels:',data.shape,labels.shape)\n",
    "    print('median:',mu.shape)\n",
    "    print('quartiles:',s1.shape,s2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# restricting normalization to training data\n",
    "# feature scaling to mitigate outliars \n",
    "if DEBUG:\n",
    "    print('normalizing training data...')\n",
    "data_norm = (data - mu) / (1 + s2 - s1)\n",
    "dataRows = data_norm.astype(np.float)\n",
    "row_length = len(dataRows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((76020, 369), (76020, 2))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataRows.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read testing data from CSV file as deep test set = dtest\n",
    "if DEBUG:\n",
    "    print('reading testing data...')\n",
    "dtest = pd.read_csv('test.csv')\n",
    "test_ids = dtest[['ID']]                     #store the ID column\n",
    "dtest.drop('ID', axis=1, inplace=True)       #discard the ID column\n",
    "dtest = dtest.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75818, 369)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize testing data - as I understand only required for NN modeling\n",
    "# It's only really required for things like Neural Networks where it keeps the gradient descent\n",
    "# of features in the space where gradient descent does best, and for Linear/Logistic Regression \n",
    "#where it also isn't really required, but makes the weights interpretable as feature importance/contribution \n",
    "#to the prediction\n",
    "\n",
    "if DEBUG:\n",
    "    print('normalizing testing data...')\n",
    "test_rows = (dtest - mu) / (1 + s2 - s1)\n",
    "test_rows = test_rows.astype(np.float)\n",
    "\n",
    "if DEBUG:\n",
    "    print('train:')\n",
    "    print(dataRows[:5])\n",
    "    print('test:')\n",
    "    print(test_rows[:5])\n",
    "    print('test_rows({0[0]},{0[1]})'.format(test_rows.shape))\n",
    "    print('row length',row_length)\n",
    "\n",
    "labels_flat = labels[:,1]\n",
    "if DEBUG:\n",
    "    print('labels_flat({0})'.format(len(labels_flat)))\n",
    "\n",
    "labels_count = np.unique(labels_flat).shape[0]\n",
    "if DEBUG:\n",
    "    print('labels_count => {0}'.format(labels_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For most classification problems \"one-hot vectors\" are used. A one-hot vector \n",
    "# is a vector that contains a single element equal to 1 and the rest of the \n",
    "# elements equal to 0. In this case, the *nth* digit is represented as a zero \n",
    "# vector with 1 in the *nth* position.\n",
    "# convert class labels from scalars to one-hot vectors\n",
    "# 0 => [1 0 0 0 0 0 0 0 0 0]\n",
    "# 1 => [0 1 0 0 0 0 0 0 0 0]\n",
    "# ...\n",
    "# 9 => [0 0 0 0 0 0 0 0 0 1]\n",
    "#\n",
    "# With a binary classification problem, but we still use \"one-hot vectors\"\n",
    "# of length 2.\n",
    "# 0 => [1 0]\n",
    "# 1 => [0 1]\n",
    "\n",
    "# NOTE TO SELF -- this function is no longer called: ------\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    i = [x+y for (x,y) in zip(index_offset,labels_dense.ravel())]\n",
    "    labels_one_hot.flat[i] = 1\n",
    "    # for some reason I needed to explicitly create the iterator i\n",
    "    # the syntax used in the original script was causing an error\n",
    "    # IndexError: unsupported iterator index\n",
    "    # it seems this may be a bug in numpy, but I haven't verified this\n",
    "    if DEBUG:\n",
    "        print('labels_one_hot',labels_one_hot.shape)\n",
    "        print('labels_one_hot.flat',labels_one_hot.flat[:].shape)\n",
    "        print('----')\n",
    "        print('index_offset',index_offset.shape)\n",
    "        print('index_offset[:5]',index_offset[:5])\n",
    "        print('labels_dense.ravel()',labels_dense.ravel().shape)\n",
    "        print('i[:10]',i[:10])\n",
    "        print('labels_dense[:5]',labels_dense[:5])\n",
    "        print('labels_one_hot[:5,:]',labels_one_hot[:5,:])\n",
    "    return labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tied_rank(x):\n",
    "    \"\"\"\n",
    "    Computes the tied rank of elements in x.\n",
    "    This function computes the tied rank of elements in x.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : list of numbers, numpy array\n",
    "    Returns\n",
    "    -------\n",
    "    score : list of numbers\n",
    "            The tied rank f each element in x\n",
    "    \"\"\"\n",
    "    x_p = [(u,i) for i,u in enumerate(x)]\n",
    "    sorted_x = sorted(x_p)\n",
    "    r = [0 for k in x]\n",
    "    cur_val = sorted_x[0][0]\n",
    "    last_rank = 0\n",
    "    for i in range(len(sorted_x)):\n",
    "        if cur_val != sorted_x[i][0]:\n",
    "            cur_val = sorted_x[i][0]\n",
    "            for j in range(last_rank, i): \n",
    "                r[sorted_x[j][1]] = float(last_rank+1+i)/2.0\n",
    "            last_rank = i\n",
    "        if i==len(sorted_x)-1:\n",
    "            for j in range(last_rank, i+1): \n",
    "                r[sorted_x[j][1]] = float(last_rank+i+2)/2.0\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auc(actual, posterior):\n",
    "    \"\"\"\n",
    "    Computes the area under the receiver-operater characteristic (AUC)\n",
    "    This function computes the AUC error metric for binary classification.\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list of binary numbers, numpy array\n",
    "             The ground truth value\n",
    "    posterior : same type as actual\n",
    "                Defines a ranking on the binary numbers, from most likely to\n",
    "                be positive to least likely to be positive.\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean squared error between actual and posterior\n",
    "    \"\"\"\n",
    "    r = tied_rank(posterior)\n",
    "    num_positive = len([0 for x in actual if x==1])\n",
    "    num_negative = len(actual)-num_positive\n",
    "    sum_positive = sum([r[i] for i in range(len(r)) if actual[i]==1])\n",
    "    auc = ((sum_positive - num_positive*(num_positive+1)/2.0) /\n",
    "           (num_negative*num_positive))\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = labels_flat #dense_to_one_hot(labels_flat, labels_count)\n",
    "labels_count = 1 # (=0 THIS Was A HACK before resetting batch size)\n",
    "labels = labels.astype(np.uint8)\n",
    "if DEBUG:\n",
    "    #print('labels({0[0]},{0[1]})'.format(labels.shape))\n",
    "    print('labels:',labels.shape)\n",
    "    print(np.sum(labels,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1], dtype=uint8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lastly we set aside data for validation. It's essential in machine\n",
    "# learning to have a separate dataset which doesn't take part in the\n",
    "# training and is used to make sure that what we've learned can actually be generalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# permute again\n",
    "n_rows = dataRows.shape[0]\n",
    "perm   = np.arange(n_rows)\n",
    "np.random.shuffle(perm)\n",
    "np.random.shuffle(perm)\n",
    "dataRows = dataRows[perm]\n",
    "labels   = labels[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into training & validation\n",
    "# (we don't allow using more than half the data for validation)\n",
    "k = min(n_rows//2,VALIDATION_SIZE)\n",
    "validation_rows = dataRows[:k]\n",
    "validation_labels = labels[:k]\n",
    "\n",
    "train_rows = dataRows[k:]\n",
    "train_labels = labels[k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((61020, 369), (61020,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rows.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# *Data is ready. The neural network structure is next.*\n",
    "# ## TensorFlow graph\n",
    "# TensorFlow does its heavy lifting outside Python. Therefore, instead of \n",
    "# running every single operation independently, TensorFlow allows users to \n",
    "# build a whole graph of interacting operations and then runs the workflow \n",
    "# in a separate process at once.\n",
    "#\n",
    "# #### Helper functions\n",
    "# For this NN model, a lot of weights and biases are created. Generally, \n",
    "# weights should most likely be initialized with a small amount of noise for symmetry \n",
    "# breaking, and to prevent 0 gradients. \n",
    "# \n",
    "# Since we are using [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) \n",
    "# neurons (ones that contain rectifier function *f(x)=max(0,x)*), it is \n",
    "# also good practice to initialize them with a slightly positive initial \n",
    "# bias to avoid \"dead neurons\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.abs(tf.truncated_normal(shape, stddev=0.01))#tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input & output of NN\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None,row_length], name='x')\n",
    "y_ = tf.placeholder(dtype=tf.float32, shape=[75,labels_count], name='y')\n",
    "# input rows\n",
    "# x = tf.placeholder('float', shape=[row_length, None])\n",
    "# y = tf.placeholder('float', shape=[75,1])\n",
    "\n",
    "# labels (output)\n",
    "# y_ = tf.placeholder('float', shape=[labels_count, None])\n",
    "\n",
    "# densely connected layer\n",
    "W_fc1 = weight_variable([row_length, HIDDEN_LAYER_SIZE[0]])\n",
    "b_fc1 = bias_variable([1,HIDDEN_LAYER_SIZE[0]])\n",
    "\n",
    "\n",
    "x_flat = x #tf.reshape(x, [None, row_length])\n",
    "\n",
    "x_fc1 = tf.sigmoid(tf.matmul(x_flat, W_fc1) + b_fc1) #tf.nn.relu(tf.matmul(x_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'truncated_normal_70:0' shape=(369, 392) dtype=float32>"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_fc1.initial_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sigmoid_33:0' shape=(?, 392) dtype=float32>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(1)]),\n",
       " TensorShape([Dimension(None), Dimension(369)]))"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To prevent NN overfitting, we  apply \n",
    "# [dropout](https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout) \n",
    "# before the readout layer.\n",
    "#some kinds of recurrent neural networks where only the parameters of the final, non-recurrent output layer (known as the readout layer) are trained\n",
    "\n",
    "# Outstanding description here: https://chatbotslife.com/regularization-in-deep-learning-f649a45d6e0     \n",
    "# Dropout removes some nodes from the network at each training stage. \n",
    "# Each of the nodes is either kept in the network with probability *keep_prob* \n",
    "# or dropped with probability *1 - keep_prob*. After the training stage is over \n",
    "# the nodes are returned to the NN with their original weights.\n",
    "#\n",
    "# dropout\n",
    "keep_prob = tf.placeholder('float')\n",
    "x_fc1_drop = tf.nn.dropout(x_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([HIDDEN_LAYER_SIZE[0],HIDDEN_LAYER_SIZE[1]])\n",
    "b_fc2 = bias_variable([1,HIDDEN_LAYER_SIZE[1]])\n",
    "\n",
    "x_fc2 = (tf.matmul(x_fc1_drop, W_fc2) + b_fc2) #tf.nn.relu(tf.matmul(x_fc1_drop, W_fc2) + b_fc2)\n",
    "x_fc2_drop = tf.nn.dropout(x_fc2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(392)]),\n",
       " <tf.Tensor 'truncated_normal_72:0' shape=(392, 169) dtype=float32>,\n",
       " <tf.Tensor 'Abs_80:0' shape=(1, 169) dtype=float32>,\n",
       " <tf.Tensor 'dropout_21/mul:0' shape=(?, 169) dtype=float32>,\n",
       " <tf.Tensor 'add_105:0' shape=(?, 169) dtype=float32>)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_fc1_drop.shape, W_fc2.initial_value, b_fc2.initial_value, x_fc2_drop, x_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, we add a softmax layer, the same one if we use just a  \n",
    "# simple [softmax regression](https://en.wikipedia.org/wiki/Softmax_function).\n",
    "#  SEE EG: http://www.ritchieng.com/machine-learning/deep-learning/tensorflow/convnets/\n",
    "# readout layer for deep net\n",
    "W_readout = weight_variable([HIDDEN_LAYER_SIZE[1], labels_count])\n",
    "b_readout = bias_variable([75,labels_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Identity_21:0' shape=(169, 1) dtype=float32>,\n",
       " <tf.Tensor 'Identity_22:0' shape=(75, 1) dtype=float32>)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " W_readout.initialized_value(), b_readout.initialized_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('x_flat', '(?, 369)'), ('W_fc1', '(369, 392)'), ('b_fc1', '(1, 392)'), ('x_fc1', '(?, 392)'), ('x_fc1_drop', '(?, 392)')]\n",
      "[('x_fc1_drop', '(?, 392)'), ('W_fc2', '(392, 169)'), ('b_fc2', '(1, 169)'), ('x_fc2', '(?, 169)'), ('x_fc2_drop', '(?, 169)')]\n",
      "[('x_fc2_drop', '(?, 169)'), ('W_readout', '(169, 1)'), ('b_readout', '(75, 1)'), ('y', '(75, 1)')]\n",
      "shape of y : (75, 1)\n",
      "shape of y_: (75, 1)\n"
     ]
    }
   ],
   "source": [
    "#y = tf.nn.sigmoid_cross_entropy_with_logits(tf.matmul(x_fc1_drop, W_fc2) + b_fc2)\n",
    "y = tf.sigmoid(tf.matmul(x_fc2_drop, W_readout) + b_readout) #tf.nn.softmax(tf.matmul(x_fc2_drop, W_readout) + b_readout)\n",
    "if DEBUG==0:\n",
    "    names = ['x_flat', 'W_fc1', 'b_fc1', 'x_fc1', 'x_fc1_drop']\n",
    "    shapes= [x_flat.get_shape(),W_fc1.get_shape(),b_fc1.get_shape(),x_fc1.get_shape(),x_fc1_drop.get_shape()]\n",
    "    print(list(zip(names,[str(x) for x in shapes])))\n",
    "    names = ['x_fc1_drop','W_fc2', 'b_fc2', 'x_fc2', 'x_fc2_drop']\n",
    "    shapes= [x_fc1_drop.get_shape(),W_fc2.get_shape(), b_fc2.get_shape(), x_fc2.get_shape(), x_fc2_drop.get_shape()]\n",
    "    print(list(zip(names,[str(x) for x in shapes])))\n",
    "    names = ['x_fc2_drop','W_readout','b_readout','y']\n",
    "    shapes= [x_fc2_drop.get_shape(),W_readout.get_shape(),b_readout.get_shape(),y.get_shape()]\n",
    "    print(list(zip(names,[str(x) for x in shapes])))\n",
    "    print('shape of y :',y.get_shape())\n",
    "    print('shape of y_:',y_.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### start of recurring (now) solved shape error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(75), Dimension(1)])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To evaluate network performance -- Cross Entropy\n",
    "# [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy) \n",
    "# and to minimise it [ADAM optimizer](http://arxiv.org/pdf/1412.6980v8.pdf) is used. \n",
    "# \n",
    "# ADAM optimizer is a gradient based optimization algorithm, based \n",
    "# on adaptive estimates, it's more sophisticated than steepest \n",
    "# gradient descent and is well suited for problems with large \n",
    "# data or many parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost functions\n",
    "# y  = tf.reshape(y,[-1])\n",
    "# y_ = tf.reshape(y_,[-1]) \n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "ones_y = tf.ones_like(y)\n",
    "ones_y_ = tf.ones_like(y_)\n",
    "log_loss = -tf.reduce_mean(y_*tf.log(tf.abs(y)+tf.scalar_mul(EPSILON,ones_y)) + (ones_y_-y_)*tf.log(tf.abs(ones_y-y)+tf.scalar_mul(EPSILON,ones_y)))\n",
    "#auc_score = -auc(y_,y) #this does not work as it is.\n",
    "#u = tf.unpack(y ,num=BATCH_SIZE)\n",
    "#v = tf.unpack(y_,num=BATCH_SIZE)\n",
    "#if DEBUG:\n",
    "#    print('unpacked y :',type(u).__name__,len(u),type(u[0]).__name__)\n",
    "#    print('unpacked y_:',type(v).__name__,len(v),type(v[0]).__name__)\n",
    "\n",
    "#u = [tf.unpack(x ,num=1) for x  in u]\n",
    "#v = [tf.unpack(x_,num=1) for x_ in v]\n",
    "#u = [x for [x] in u]\n",
    "#v = [x for [x] in v]\n",
    "#if DEBUG:\n",
    "#    print('unpacked y :',type(u).__name__,len(u),type(u[0]).__name__)\n",
    "#    print('unpacked y_:',type(v).__name__,len(v),type(v[0]).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sigmoid_34:0' shape=(75, 1) dtype=float32>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimization function: ULTIMATELY GO W/ GRADIENT DESCENT\n",
    "#train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(log_loss)\n",
    "# evaluation\n",
    "# correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float')) #original\n",
    "# accuracy = log_loss\n",
    "\n",
    "ones_y = tf.ones_like(y)\n",
    "ones_y_ = tf.ones_like(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#accuracy = -auc(y_,y) # = this does not work & all other cell-mates too!\n",
    "accuracy = tf.contrib.metrics.accuracy(tf.argmax(ones_y_,axis=1), tf.argmax(ones_y,axis=1))\n",
    "# accuracy = -tf.reduce_mean(y_*tf.log(tf.abs(y)+tf.scalar_mul(EPSILON,ones_y)) + (ones_y_-y_)*tf.log(tf.abs(ones_y-y)+tf.scalar_mul(EPSILON,ones_y)))\n",
    "#accuracy = -tf.reduce_mean(y_*tf.log(tf.abs(y)+EPSILON) + (tf.ones_like(y_)-y_)*tf.log(tf.abs(tf.ones_like(y)-y))+EPSILON)\n",
    "#accuracy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "batch_xs, batch_ys = next_batch(BATCH_SIZE)\n",
    "print(batch_ys.flatten())\n",
    "#batch_ys = np.reshape(batch_ys,(batch_ys.size,1))\n",
    "\n",
    "# check progress on every 1st,2nd,...,10th,20th,...,100th... step\n",
    "# if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n",
    "\n",
    "train_accuracy = accuracy.eval(feed_dict={x:batch_xs,y_: batch_ys,keep_prob: 1.0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75, 369), (75, 1))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_xs.shape, batch_ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To predict values from test data, highest probability selected from\n",
    "# \"one-hot vector\" \n",
    "# indicates probability of digits being of highest value\n",
    "\n",
    "# prediction function\n",
    "# [0.1, 0.9, 0.2, 0.1, 0.1 0.3, 0.5, 0.1, 0.2, 0.3] => 1\n",
    "\n",
    "predict = tf.scalar_mul(1.0,tf.reshape(y,[-1])) # prediction probabilities\n",
    "# predict = tf.scalar_mul(1.0, y[:,1])    # prediction probabilities\n",
    "# predict = tf.identity(y[:,1]) #y[:,1] # alt prediction probabilities\n",
    "# predict = tf.argmax(y,1) # Or, in this case: [0.4, 0.6] => 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining NN structure w/ TensorFlow graph\n",
    "# TRAINING -------------\n",
    "# ## Train, validate and predict\n",
    "# #### Helper functions\n",
    "# \n",
    "# Ideally, should use all data for every step of the training, but \n",
    "# that's computationally expensive. \n",
    "\n",
    "#  Alternativly, use small \"batches\" of random data. \n",
    "# \n",
    "# Applying a method called:\n",
    "# [stochastic training](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). \n",
    "# This approach is theoretically cheaper, faster and gives much of the same result.\n",
    "\n",
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = train_rows.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply batched data:\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global train_rows\n",
    "    global train_labels\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when entirety of training data expent, reorder randomly therafter   \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_rows = train_rows[perm]\n",
    "        train_labels = train_labels[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        if DEBUG:\n",
    "            print('batch_size,num_examples:',batch_size,num_examples)\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    s = train_labels[start:end].size\n",
    "    shaped_y = np.reshape(train_labels[start:end],(s,1))\n",
    "    return train_rows[start:end],shaped_y #train_labels[start:end] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now when all operations for every variable are defined in TensorFlow \n",
    "# Thereafter graph all computations will be performed outside Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-304-9491634cf160>:2: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "# start TensorFlow session -----------------\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each step of the loop, fetch a \"batch\" of data points from the training set\n",
    "# and feed to the graph to replace the placeholders.   \n",
    "#  For this case, it's:  *x, y* and *dropout.*\n",
    "# \n",
    "# Additionally, must periodically check training accuracy for an upcoming \"batch\".\n",
    "# \n",
    "# On the local environment, we recommend [saving training progress]\n",
    "# (https://www.tensorflow.org/versions/master/api_docs/python/state_ops.html#Saver),\n",
    "# so it can be recovered for further training, debugging or evaluation.\n",
    "# visualisation variables\n",
    "train_accuracies = []\n",
    "validation_accuracies = []\n",
    "x_range = []\n",
    "\n",
    "display_step=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TENSORFLOW DEBUGGING GUIDE:  https://wookayin.github.io/tensorflow-talk-debugging/#1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING SET & TEST SETS \n",
    "## RE-ADJUSTED TO MATCH UP WITH MINIBATCH SETUP\n",
    "### ULTIMATELY avoided long-standing \"shape error\" shown in\n",
    "### (shown in preceeding Milestone No. 3 for this ML2 Project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 369)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_rows[0:BATCH_SIZE].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 0\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 1\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 2\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 3\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 4\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 5\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 6\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 7\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 8\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 9\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 10\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 20\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 30\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 40\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 50\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 60\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 70\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 80\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 90\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 100\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 200\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 300\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 400\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 500\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 600\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 700\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 800\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 900\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 1000\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 2000\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 3000\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 4000\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 5000\n",
      "training_accuracy / validation_accuracy => 1.00 / 1.00 for step 5999\n"
     ]
    }
   ],
   "source": [
    "for i in range(TRAINING_ITERATIONS):\n",
    "    #get new batch\n",
    "    batch_xs, batch_ys = next_batch(BATCH_SIZE)\n",
    "    #batch_ys = np.reshape(batch_ys,(batch_ys.size,1))\n",
    "\n",
    "    # check progress on every 1st,2nd,...,10th,20th,...,100th... step\n",
    "    if i%display_step == 0 or (i+1) == TRAINING_ITERATIONS:\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_xs,y_: batch_ys,keep_prob: 1.0})\n",
    "        if(VALIDATION_SIZE):\n",
    "            true_ys = np.reshape(validation_labels[0:BATCH_SIZE],(BATCH_SIZE,1))\n",
    "            validation_accuracy = accuracy.eval(feed_dict={x:validation_rows[0:BATCH_SIZE],y_: true_ys,keep_prob: 1.0})\n",
    "            print('training_accuracy / validation_accuracy => %.2f / %.2f for step %d'%(train_accuracy, validation_accuracy, i))\n",
    "            validation_accuracies.append(validation_accuracy)\n",
    "        else:\n",
    "            print('training_accuracy => %.4f for step %d'%(train_accuracy, i))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        x_range.append(i)\n",
    "        \n",
    "        # increase display_step\n",
    "        if i%(display_step*10) == 0 and i:\n",
    "            display_step *= 10\n",
    "    # train on batch\n",
    "    if DEBUG:\n",
    "        print(batch_xs.shape,batch_ys.shape)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: DROPOUT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# On Completion of training, check accuracy on data that \n",
    "#  was not used for training.\n",
    "# check final accuracy on validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation_accuracy => 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNaIwO3MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12c0e0f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if(VALIDATION_SIZE):\n",
    "    true_ys = np.reshape(validation_labels,(validation_labels.size,1))\n",
    "    validation_accuracy = accuracy.eval(feed_dict={x: validation_rows[:75], y_: true_ys[:75], keep_prob: 1.0})\n",
    "    print('validation_accuracy => %.4f'%validation_accuracy)\n",
    "    plt.plot(x_range, train_accuracies,'-b', label='Training')\n",
    "    plt.plot(x_range, validation_accuracies,'-g', label='Validation')\n",
    "    plt.legend(loc='lower right', frameon=False)\n",
    "    plt.ylim(ymax = 1.1, ymin = -0.1)\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('step')\n",
    "    if GRAPH:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict test set\n",
    "predicted_labels = predict.eval(feed_dict={x: test_rows[:75], keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch hyperparameter is more resource efficient\n",
    "predicted_labels = np.zeros(test_rows.shape[0])\n",
    "for i in range(0,test_rows.shape[0]//BATCH_SIZE):\n",
    "    predicted_labels[i*BATCH_SIZE : (i+1)*BATCH_SIZE] = predict.eval(feed_dict={\n",
    "                       x: test_rows[i*BATCH_SIZE : (i+1)*BATCH_SIZE], keep_prob: 1.0})\n",
    "\n",
    "if DEBUG:\n",
    "    print('predicted_labels({0})'.format(len(predicted_labels)))\n",
    "    print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SATURDAY SPECIAL EDITION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](coding_Cat.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This addition in direct response to my Thursday 20 July in-class presentation.  Although a Data Dictionary was not provided with this dataset, I followed the next best thing -- predict on the most important features of the dataset(although it is unknown what they ultimately mean).  Thus, if a dictionary was provided in the future, we would get the accuracy on the most important features on the dataset. -- rb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_train = pd.read_csv(\"train.csv\", index_col='ID')\n",
    "feature_cols = list(df_train.columns)\n",
    "feature_cols.remove(\"TARGET\")\n",
    "df_test = pd.read_csv(\"test.csv\", index_col='ID')\n",
    "\n",
    "# Split up the data\n",
    "X_all = df_train[feature_cols]\n",
    "y_all = df_train[\"TARGET\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.3, random_state=5, stratify=y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 65\n",
      "AUC Score (Train): 0.867708\n",
      "AUC Score (Test) : 0.840538\n"
     ]
    }
   ],
   "source": [
    "# Get top features from xgb model\n",
    "model = xgb.XGBRegressor(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=9,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=5\n",
    ")\n",
    "\n",
    "# Train cv\n",
    "xgb_param = model.get_xgb_params()\n",
    "dtrain = xgb.DMatrix(X_train.values, label=y_train.values, missing=np.nan)\n",
    "cv_result = xgb.cv(\n",
    "    xgb_param,\n",
    "    dtrain,\n",
    "    num_boost_round=model.get_params()['n_estimators'],\n",
    "    nfold=5,\n",
    "    metrics=['auc'],\n",
    "    early_stopping_rounds=50)\n",
    "best_n_estimators = cv_result.shape[0]\n",
    "model.set_params(n_estimators=best_n_estimators)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "# Predict training data\n",
    "y_hat_train = model.predict(X_train)\n",
    "\n",
    "# Predict test data\n",
    "y_hat_test = model.predict(X_test)\n",
    "\n",
    "# Print model report:\n",
    "print(\"\\nModel Report\")\n",
    "print(\"best n_estimators: {}\".format(best_n_estimators))\n",
    "print(\"AUC Score (Train): %f\" % roc_auc_score(y_train, y_hat_train))\n",
    "print(\"AUC Score (Test) : %f\" % roc_auc_score(y_test,  y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get important features\n",
    "feat_imp = list(pd.Series(model.booster().get_fscore()).sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/x/anaconda/envs/dl/lib/python3.6/site-packages/sklearn/preprocessing/data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "# Even out the targets\n",
    "df_train_1 = df_train[df_train[\"TARGET\"] == 1]\n",
    "df_train_0 = df_train[df_train[\"TARGET\"] == 0].head(df_train_1.shape[0])\n",
    "df_train = df_train_1.append(df_train_0)\n",
    "\n",
    "# Scale data\n",
    "X_all = df_train[feat_imp].copy(deep=True)\n",
    "y_all = df_train[\"TARGET\"]\n",
    "X_all[feat_imp] = sklearn.preprocessing.scale(X_all, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.3, random_state=5, stratify=y_all)\n",
    "\n",
    "# Create second complementary column at position 0\n",
    "y_train_2cols = np.array(list(zip((1 - y_train).values, y_train.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-26a820dd3ade>:87: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Batch loss at step 0: 0.717138\n",
      "Batch score: 0.539554\n",
      "Test score: 0.510889\n",
      "Batch loss at step 500: 0.604797\n",
      "Batch score: 0.746547\n",
      "Test score: 0.786932\n",
      "Batch loss at step 1000: 0.580655\n",
      "Batch score: 0.747061\n",
      "Test score: 0.802625\n",
      "Batch loss at step 1500: 0.620887\n",
      "Batch score: 0.741010\n",
      "Test score: 0.807610\n",
      "Batch loss at step 2000: 0.596551\n",
      "Batch score: 0.772100\n",
      "Test score: 0.810459\n",
      "Batch loss at step 2500: 0.614841\n",
      "Batch score: 0.746477\n",
      "Test score: 0.811980\n",
      "Batch loss at step 3000: 0.544632\n",
      "Batch score: 0.808586\n",
      "Test score: 0.811915\n",
      "Final Test score: 0.811915\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Hyperparameters\n",
    "n_steps = 3001\n",
    "batch_size = 200\n",
    "learning_rate0 = 0.05\n",
    "decay_steps = 500\n",
    "decay_rate = 0.8\n",
    "\n",
    "# Network parameters\n",
    "n_h1 = 20\n",
    "n_h2 = 20\n",
    "n_features = X_train.shape[1]\n",
    "n_labels = 2\n",
    "\n",
    "# L2 regularization\n",
    "beta = 1e-5\n",
    "\n",
    "# Dropout\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Allocate variables\n",
    "    tf_X_train = tf.placeholder(tf.float32, shape=(batch_size, n_features))\n",
    "    tf_y_train = tf.placeholder(tf.float32, shape=(batch_size, n_labels))\n",
    "    tf_X_test = tf.constant(X_test.values, dtype=tf.float32)\n",
    "    \n",
    "    tf_keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Hidden layer\n",
    "    with tf.name_scope('h1') as scope:\n",
    "        weights_h1 = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [n_features, n_h1],\n",
    "                stddev=1.0 / np.sqrt(n_features)\n",
    "        ), name='weights_h1')\n",
    "        biases_h1 = tf.Variable(tf.zeros([n_h1]), name='biases_h1')        \n",
    "        h1 = tf.nn.relu(tf.matmul(tf_X_train, weights_h1) + biases_h1)\n",
    "\n",
    "        # Dropout\n",
    "        h1 = tf.nn.dropout(h1, tf_keep_prob)\n",
    "        \n",
    "    # Hidden layer 2\n",
    "    with tf.name_scope('h2') as scope:\n",
    "        weights_h2 = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [n_h1, n_h2],\n",
    "                stddev=1.0 / np.sqrt(n_h1)\n",
    "        ), name='weights_h2')\n",
    "        biases_h2 = tf.Variable(tf.zeros([n_h2]), name='biases_h2')\n",
    "        h2 = tf.nn.relu(tf.matmul(h1, weights_h2) + biases_h2)\n",
    "\n",
    "        # Dropout\n",
    "        h2 = tf.nn.dropout(h2, tf_keep_prob)\n",
    "        \n",
    "    # Output layer\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights_out = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [n_h2, n_labels],\n",
    "                stddev=1.0 / np.sqrt(n_h2)\n",
    "            ), name='weights_out')\n",
    "        biases_out = tf.Variable(tf.zeros([n_labels]), name='biases_out')\n",
    "        logits = tf.matmul(h2, weights_out) + biases_out\n",
    "\n",
    "    # Training computation.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_y_train))\n",
    "    \n",
    "    # L2 Regularization\n",
    "    #loss += beta * (tf.nn.l2_loss(weights_h1) + tf.nn.l2_loss(weights_h2) + tf.nn.l2_loss(weights_out))\n",
    "\n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate0, global_step, decay_steps, decay_rate, staircase=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training and test datasets.\n",
    "    yhat_train = tf.nn.softmax(logits)\n",
    "    yhat_test = tf.nn.relu(tf.matmul(tf_X_test, weights_h1) + biases_h1)\n",
    "    yhat_test = tf.nn.relu(tf.matmul(yhat_test, weights_h2) + biases_h2)\n",
    "    yhat_test = tf.nn.softmax(tf.matmul(yhat_test, weights_out) + biases_out)\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(n_steps):    \n",
    "        _batch_idx = np.random.choice(X_train.shape[0], size=batch_size, replace=False)\n",
    "        batch_X = X_train.values[_batch_idx, :]\n",
    "        batch_y = y_train_2cols[_batch_idx, :]\n",
    "\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        feed_dict = {\n",
    "            tf_X_train: batch_X,\n",
    "            tf_y_train: batch_y,\n",
    "            tf_keep_prob: keep_prob\n",
    "        }\n",
    "        _, l, pred = session.run([optimizer, loss, yhat_train], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Batch loss at step {0:d}: {1:.6f}\".format(step, l))\n",
    "            print(\"Batch score: {0:.6f}\".format(roc_auc_score(batch_y[:, 1], pred[:, 1])))\n",
    "            print(\"Test score: {0:.6f}\".format(roc_auc_score(y_test.values, yhat_test.eval()[:, 1])))\n",
    "    print(\"Final Test score: {0:.6f}\".format(roc_auc_score(y_test.values, yhat_test.eval()[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP predictions on Important Features:\n",
    "XGBoost was used as a tool to select the Important Fetures from the given dataset.  Thereafter, Tensorflow help predict on those selected features. The concluding final accuracy on the prediction is 0.81 -- which is still not as accurate at the XGBoost model as a stand alone ML model.  \n",
    "\n",
    "This again, confirms the notion, that NNs are not necessary the end-all solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](spanishMap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative -\n",
    "# Tensorflow Wrapper Exploration\n",
    "# XG BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "xgdf_train = pd.read_csv('train.csv', index_col='ID')\n",
    "feature_cols = list(xgdf_train.columns)\n",
    "feature_cols.remove(\"TARGET\")\n",
    "xgdf_test = pd.read_csv('test.csv', index_col='ID')\n",
    "\n",
    "# Split up the data\n",
    "X_all = xgdf_train[feature_cols]\n",
    "y_all = xgdf_train[\"TARGET\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.3, random_state=5, stratify=y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get top features from xgb model\n",
    "model = xgb.XGBRegressor(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=9,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=0.8,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=9, missing=None, n_estimators=65, nthread=4,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=5, silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train cv\n",
    "xgb_param = model.get_xgb_params()\n",
    "dtrain = xgb.DMatrix(X_train.values, label=y_train.values, missing=np.nan)\n",
    "cv_result = xgb.cv(\n",
    "    xgb_param,\n",
    "    dtrain,\n",
    "    num_boost_round=model.get_params()['n_estimators'],\n",
    "    nfold=5,\n",
    "    metrics=['auc'],\n",
    "    early_stopping_rounds=50)\n",
    "best_n_estimators = cv_result.shape[0]\n",
    "model.set_params(n_estimators=best_n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "# Predict training data\n",
    "y_hat_train = model.predict(X_train)\n",
    "\n",
    "# Predict test data\n",
    "y_hat_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "best n_estimators: 65\n",
      "AUC Score (Train): 0.867708\n",
      "AUC Score (Test) : 0.840538\n"
     ]
    }
   ],
   "source": [
    "# Print model report:\n",
    "print(\"\\nModel Report\")\n",
    "print(\"best n_estimators: {}\".format(best_n_estimators))\n",
    "print(\"AUC Score (Train): %f\" % roc_auc_score(y_train, y_hat_train))\n",
    "print(\"AUC Score (Test) : %f\" % roc_auc_score(y_test,  y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get important features\n",
    "feat_imp = list(pd.Series(model.booster().get_fscore()).sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/x/anaconda/envs/dl/lib/python3.6/site-packages/sklearn/preprocessing/data.py:160: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "# Even out the targets\n",
    "df_train_1 = xgdf_train[xgdf_train[\"TARGET\"] == 1]\n",
    "df_train_0 = xgdf_train[xgdf_train[\"TARGET\"] == 0].head(df_train_1.shape[0])\n",
    "xgdf_train = df_train_1.append(df_train_0)\n",
    "\n",
    "# Scale data\n",
    "X_all = xgdf_train[feat_imp].copy(deep=True)\n",
    "y_all = xgdf_train[\"TARGET\"]\n",
    "X_all[feat_imp] = sklearn.preprocessing.scale(X_all, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.3, random_state=5, stratify=y_all)\n",
    "\n",
    "# Create second complementary column at position 0\n",
    "y_train_2cols = np.array(list(zip((1 - y_train).values, y_train.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_steps = 3001\n",
    "batch_size = 200\n",
    "learning_rate0 = 0.05\n",
    "decay_steps = 500\n",
    "decay_rate = 0.8\n",
    "\n",
    "# Network parameters\n",
    "n_h1 = 20\n",
    "n_h2 = 20\n",
    "n_features = X_train.shape[1]\n",
    "n_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# L2 regularization\n",
    "beta = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-2011e1eca711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m    \u001b[0;31m# Training computation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# L2 Regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/x/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1576\u001b[0m   \"\"\"\n\u001b[1;32m   1577\u001b[0m   _ensure_xent_args(\"softmax_cross_entropy_with_logits\", _sentinel,\n\u001b[0;32m-> 1578\u001b[0;31m                     labels, logits)\n\u001b[0m\u001b[1;32m   1579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m   \u001b[0;31m# TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/x/anaconda/envs/dl/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_ensure_xent_args\u001b[0;34m(name, sentinel, labels, logits)\u001b[0m\n\u001b[1;32m   1531\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m     raise ValueError(\"Only call `%s` with \"\n\u001b[0;32m-> 1533\u001b[0;31m                      \"named arguments (labels=..., logits=..., ...)\" % name)\n\u001b[0m\u001b[1;32m   1534\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1535\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Both labels and logits must be provided.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"
     ]
    }
   ],
   "source": [
    "# Tensorflow Model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Hyperparameters\n",
    "n_steps = 3001\n",
    "batch_size = 200\n",
    "learning_rate0 = 0.05\n",
    "decay_steps = 500\n",
    "decay_rate = 0.8\n",
    "\n",
    "# Network parameters\n",
    "n_h1 = 20\n",
    "n_h2 = 20\n",
    "n_features = X_train.shape[1]\n",
    "n_labels = 2\n",
    "\n",
    "# L2 regularization\n",
    "beta = 1e-5\n",
    "\n",
    "# Dropout\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Allocate variables\n",
    "    tf_X_train = tf.placeholder(tf.float32, shape=(batch_size, n_features))\n",
    "    tf_y_train = tf.placeholder(tf.float32, shape=(batch_size, n_labels))\n",
    "    tf_X_test = tf.constant(X_test.values, dtype=tf.float32)\n",
    "    \n",
    "    tf_keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Hidden layer\n",
    "    with tf.name_scope('h1') as scope:\n",
    "        weights_h1 = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [n_features, n_h1],\n",
    "                stddev=1.0 / np.sqrt(n_features)\n",
    "        ), name='weights_h1')\n",
    "        biases_h1 = tf.Variable(tf.zeros([n_h1]), name='biases_h1')        \n",
    "        h1 = tf.nn.relu(tf.matmul(tf_X_train, weights_h1) + biases_h1)\n",
    "\n",
    "        # Dropout\n",
    "        h1 = tf.nn.dropout(h1, tf_keep_prob)\n",
    "        \n",
    "    # Hidden layer 2\n",
    "    with tf.name_scope('h2') as scope:\n",
    "        weights_h2 = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [n_h1, n_h2],\n",
    "                stddev=1.0 / np.sqrt(n_h1)\n",
    "        ), name='weights_h2')\n",
    "        biases_h2 = tf.Variable(tf.zeros([n_h2]), name='biases_h2')\n",
    "        h2 = tf.nn.relu(tf.matmul(h1, weights_h2) + biases_h2)\n",
    "\n",
    "        # Dropout\n",
    "        h2 = tf.nn.dropout(h2, tf_keep_prob)\n",
    "        \n",
    "    # Output layer\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights_out = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                [n_h2, n_labels],\n",
    "                stddev=1.0 / np.sqrt(n_h2)\n",
    "            ), name='weights_out')\n",
    "        biases_out = tf.Variable(tf.zeros([n_labels]), name='biases_out')\n",
    "        logits = tf.matmul(h2, weights_out) + biases_out\n",
    "\n",
    "   # Training computation.\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_y_train))\n",
    "    \n",
    "    # L2 Regularization\n",
    "    #loss += beta * (tf.nn.l2_loss(weights_h1) + tf.nn.l2_loss(weights_h2) + tf.nn.l2_loss(weights_out))\n",
    "\n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        learning_rate0, global_step, decay_steps, decay_rate, staircase=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training and test datasets.\n",
    "    yhat_train = tf.nn.softmax(logits)\n",
    "    yhat_test = tf.nn.relu(tf.matmul(tf_X_test, weights_h1) + biases_h1)\n",
    "    yhat_test = tf.nn.relu(tf.matmul(yhat_test, weights_h2) + biases_h2)\n",
    "    yhat_test = tf.nn.softmax(tf.matmul(yhat_test, weights_out) + biases_out)\n",
    "    \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(n_steps):    \n",
    "        _batch_idx = np.random.choice(X_train.shape[0], size=batch_size, replace=False)\n",
    "        batch_X = X_train.values[_batch_idx, :]\n",
    "        batch_y = y_train_2cols[_batch_idx, :]\n",
    "\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        feed_dict = {\n",
    "            tf_X_train: batch_X,\n",
    "            tf_y_train: batch_y,\n",
    "            tf_keep_prob: keep_prob\n",
    "        }\n",
    "        _, l, pred = session.run([optimizer, loss, yhat_train], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Batch loss at step {0:d}: {1:.6f}\".format(step, l))\n",
    "            print(\"Batch score: {0:.6f}\".format(roc_auc_score(batch_y[:, 1], pred[:, 1])))\n",
    "            print(\"Test score: {0:.6f}\".format(roc_auc_score(y_test.values, yhat_test.eval()[:, 1])))\n",
    "    print(\"Final Test score: {0:.6f}\".format(roc_auc_score(y_test.values, yhat_test.eval()[:, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](altaMira.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The process if applying different models for use with the Santander dataset is more about the journey of exploration in machine learning as opposed to searching for a neural network model as the best model for prediction.  Many dos and don'ts were explored along the way\n",
    "\n",
    "\n",
    "Indeed, the questions that are posed with this project help crystalize that greater understanding of machine learning processes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "WHAT WAS THE BIGGEST CHALLENGE/OBSTACLE HURDLE? DATA? MODELING? TRAINING?\n",
    "\n",
    "Given hindsight, the biggest obstacle was the data.  The dataset was relatively small and not ideally suited for a neural network model.  Although “top features” were extracted by given packages, there was no index or \"Data Dictionary\" given to accompany the features with the dataset to gain further insight on the data itself.    A general understanding is that finding good inputs and collecting enough training data take considerable time and effort than training a neural network.  \n",
    "\n",
    "Moreover, understanding batch sizes for training sets appears to require some mystical black magic that I hope to decipher with greater experience.  Through some rough experience above, I found smaller batch sizes to work for me for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF YOU HAD TWO MORE WEEKS - WHAT WOULD YOU DO?\n",
    "\n",
    "If given two more weeks, I would spend considerably more time to understand the dataset better.  With a better understanding upfront, I would feel more confident with satisfactorily tuning the hyperparameters.  Moreover, I would be more cognizant of overfitting the training data for the nn MLP model that I was working on so as to improve the general model.  The XGBoost model also requires a better, experienced intuition with higher variance and lower bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOES IT SCALE?\n",
    "\n",
    "The best model for the dataset was determined to be the XGBoost.  As XGBoost’s base learners are trees, any monotonic function of any feature variable will have not effect on how the trees are formed.  Thus, scaling for XGBoost is unnecessary.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "HOW WOULD YOU TURN YOUR PROJECT TO A DATA PROJECT?\n",
    "\n",
    "The XGBoost model developed with this project would work well in mitigating or eliminating customer churn by Santander’s online and offline customers.  The derived model can facilitate targeting marketing to potentially churning or dissatisfied customers in the effort to keep their business by personal improving their customer experience with targeted bank promotions and improved positive experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "WHY DOES YOUR DEEP LEARNING MODEL BEAT YOUR BASELINE?\n",
    "\n",
    "NO- the MLP deep learning model does not beat the baseline model of XGBoost.  XGBoost does a great job for the given Santander Banking data.  From what I best understand by the literature, when you don’t have large amounts of training examples, like in the 100k to 1000k range, then those alternatives to deep learning methods are superior in performance (as they don’t overfit, are often computationally superior to begin with). Thus, the Santander customer satisfaction data set of about 76k observations and about 300 features is ideally suited for machine learning as opposed to deep learning.  Illustratively, as the Santander dataset lacks image data and rich text data, deep learning models are not necessary.  It should be added that the Santander data set had already converted many text sentiment to some characteristic numerical representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHAT ARE YOUR TRADEOFFS BETWEEN YOUR BASELINE AND YOUR DEEP LEARNING MODEL? WOULD YOU PUT YOUR DEEP LEARNING MODEL INTO PRODUCTION OVER YOUR BASELINE? WHY OR WHY NOT?\n",
    "\n",
    "As the Santander dataset ultimately seeks classification as either favorable or unfavorable customer response, the balance of tradeoffs between a baseline machine learning model and a deep learning model are quite stark, and greatly depends on the what solutions are specifically being sought.  Given the predefined features already provided by the Santander dataset, classification is straightforward and requires a minimal computational footprint especially because much of the text was already numerically converted.  The standard baseline model, such as with XGBoost, is ideal for commercial production because it provides the needed sentiment classification at minimal computational power and speed – therefore achieving quick answers at low cost.\n",
    "On the other hand, a deep model would have been ideal if Santander would have alternatively provided characteristically different data for sentiment classification such as: 1) image data of actual customers either happily or angrily interacting with their bank app or atms; or 2) a vast Santander datalake full of Tweets, Instagram or IoT/wearable harvested data in many languages that Santander bank services for customer classification satisfaction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHAT EVIDENCE CAN YOU PROVIDE THAT YOUR MODEL HAS GENERALIZED CORRECTLY?\n",
    "\n",
    "If the output of the test case is close to that of the past training cases, this should provide evidence that my model has generalized correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARE YOUR RESULTS SIGNIFICANT?\n",
    "\n",
    "Assuming the mathematical sense of \"significant\":\n",
    "1) Yes, the results are statistically significant as my model results are not attibutable to chance.\n",
    "2) Arbuably yes, the results are practically significant based on the size of the difference is meaningful - in that I was able to successfully get my NN tensorflow model to run with smaller batch sizes with results that do indeed overcome underlying noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHAT HYPERPARAMETERS MATTERED? WHICH DIDN'T?  \n",
    "\n",
    "Here's a laundry list of some hyperparameters that I can identify:  Batch Size, Training Epochs, Learning Rates, Dropout, Convolutional size, number of layers.  \n",
    "WHAT HYPERPARAMETERS MATTERED?  A great way to test for this is to adjust one hyperparameter at a time so that the training output becomes less accurate.  I found my model only worked with smaller batch sizes but training accuracy decreased with decreasing size most likely due to noise.  Training epochs and learning rates tended not to have such a strong effect on the entire model as batch size (at least in the Tensorflow universe).\n",
    "\n",
    "WHICH DIDN'T? Layer Numbers in that I did not experiment with multiple numbers for my model.  Dropout was too problematical for my model to execute as well although it is a nice and easy way to prevent overfitting.  In action, a dropout layer randomly removes some nodes in the network permitting the nodes to become less sensitive and work robustly well with different combinations within the hidden layer.  Recall the wonderful analogy of the construction workers who randomly called in sick:  https://chatbotslife.com/regularization-in-deep-learning-f649a45d6e0 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHY DID OR DIDN'T YOU USE ACCURACY AS YOUR EVALUATION METRIC?\n",
    "\n",
    "I used accuracy as my evaluation metric to afford greater paredy between my tensorflow model and sklearn model or sklearn ensemble results. For evaluating any classifier is the confusion matrix as Accuracy, precision, and F-score is captured.  Other metrics are area under the ROC curve and area under the ROC curve. (PS: during this reflection, I've just realized that I have applied the same formula to loss and accuracy in my tensorflow model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](portSantander.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Thank you for your interest and for a fun semester!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
